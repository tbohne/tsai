{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp data.validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">Functions required to perform cross-validation and transform unique time series sequence into multiple samples ready to be used by a time series model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from tsai.imports import *\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from fastcore.xtras import is_listy\n",
    "from tsai.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RandomSplitter(valid_pct=0.2, seed=None):\n",
    "    \"Create function that splits `items` between train/val with `valid_pct` randomly.\"\n",
    "    def _inner(o):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        rand_idx = L(list(torch.randperm(len(o)).numpy()))\n",
    "        cut = int(valid_pct * len(o))\n",
    "        return rand_idx[cut:],rand_idx[:cut]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def check_overlap(a, b, c=None):\n",
    "    \"Checks if there's overlap between array-like objects\"\n",
    "    a = np.asarray(a).flatten()\n",
    "    b = np.asarray(b).flatten()\n",
    "    c = np.asarray(c).flatten() if c is not None else c\n",
    "    ab = np.isin(a, b)\n",
    "    ac = np.isin(a, c) if c is not None else np.array([False])\n",
    "    bc = np.isin(b, c) if c is not None else np.array([False])\n",
    "    if ab.sum() + ac.sum() + bc.sum() == 0: return False\n",
    "    if c is None: return L(a[ab].tolist())\n",
    "    return L(a[ab].tolist()), L(a[ac].tolist()), L(b[bc].tolist())\n",
    "\n",
    "def check_splits_overlap(splits):\n",
    "    return [check_overlap(*_splits) for _splits in splits] if is_listy(splits[0][0]) else check_overlap(*splits)\n",
    "\n",
    "def leakage_finder(*splits, verbose=True):\n",
    "    '''You can pass splits as a tuple, or train, valid, ...'''\n",
    "    splits = L(*splits)\n",
    "    overlaps = 0\n",
    "    for i in range(len(splits)):\n",
    "        for j in range(i + 1, len(splits)):\n",
    "            overlap = check_overlap(splits[i], splits[j])\n",
    "            if overlap: \n",
    "                pv(f'overlap between splits [{i}, {j}] {overlap}', verbose)\n",
    "                overlaps += 1\n",
    "    assert overlaps == 0, 'Please, review your splits!'\n",
    "\n",
    "def balance_idx(o, shuffle=False, strategy=\"oversample\", random_state=None, verbose=False):\n",
    "    assert strategy in [\"oversample\", \"undersample\"]\n",
    "    if isinstance(o, list): o = L(o)\n",
    "    idx_ = np.arange(len(o)).reshape(-1, 1)\n",
    "    if strategy == \"oversample\":\n",
    "        ros = RandomOverSampler(random_state=random_state)\n",
    "    elif strategy == \"undersample\":\n",
    "        ros = RandomUnderSampler(random_state=random_state)\n",
    "    resampled_idxs, _ = ros.fit_resample(idx_, np.asarray(o))\n",
    "    new_idx = L(resampled_idxs.reshape(-1,).tolist())\n",
    "    if shuffle: new_idx = random_shuffle(new_idx)\n",
    "    return new_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10)\n",
    "b = np.arange(10, 20)\n",
    "test_eq(check_overlap(a, b), False)\n",
    "a = np.arange(10)\n",
    "b = np.arange(9, 20)\n",
    "test_eq(check_overlap(a, b), [9])\n",
    "a = np.arange(10)\n",
    "b = np.arange(10, 20)\n",
    "c = np.arange(20, 30)\n",
    "test_eq(check_overlap(a, b, c), False)\n",
    "a = np.arange(10)\n",
    "b = np.arange(10, 20)\n",
    "c = np.arange(10, 30)\n",
    "test_eq(check_overlap(a, b, c), ([], [], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([[i] * np.random.randint(10, 100) for i in range(5)])\n",
    "train_split = np.random.choice(len(y), int(len(y) * .8), False)\n",
    "c, v = np.unique(y[train_split], return_counts=True)\n",
    "print(f\"{'imbalanced:':25} {c} {v}\")\n",
    "\n",
    "oversampled_train_split = train_split[balance_idx(y[train_split], strategy=\"oversample\")]\n",
    "osc, osv = np.unique(y[oversampled_train_split], return_counts=True)\n",
    "print(f\"{'balanced (oversample):':25} {osc} {osv}\")\n",
    "test_eq(osv, [max(v)] * len(v))\n",
    "\n",
    "undersampled_train_split = train_split[balance_idx(y[train_split], strategy=\"undersample\")]\n",
    "usc, usv = np.unique(y[undersampled_train_split], return_counts=True)\n",
    "print(f\"{'balanced (undersample):':25} {usc} {usv}\")\n",
    "test_eq(usv, [min(v)] * len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = L(list(concat(np.zeros(5), np.ones(10)).astype(int)))\n",
    "balanced_idx = balance_idx(l)\n",
    "test_eq(np.mean(l[balanced_idx]), 0.5)\n",
    "test_eq(isinstance(balanced_idx, L), True)\n",
    "\n",
    "l = list(concat(np.zeros(5), np.ones(10)).astype(int))\n",
    "balanced_idx = balance_idx(l)\n",
    "test_eq(np.mean(L(l)[balanced_idx]), 0.5)\n",
    "test_eq(isinstance(balanced_idx, L), True)\n",
    "\n",
    "a = concat(np.zeros(5), np.ones(10)).astype(int)\n",
    "balanced_idx = balance_idx(a)\n",
    "test_eq(np.mean(a[balanced_idx]), 0.5)\n",
    "test_eq(isinstance(balanced_idx, L), True)\n",
    "\n",
    "t = concat(torch.zeros(5), torch.ones(10))\n",
    "balanced_idx = balance_idx(t, shuffle=True)\n",
    "test_eq(t[balanced_idx].mean(), 0.5)\n",
    "test_eq(isinstance(balanced_idx, L), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.arange(100_000), np.arange(100_000, 200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_labels = True\n",
    "filter_pseudolabels = .5\n",
    "balanced_pseudolabels = True\n",
    "\n",
    "pseudolabels = torch.rand(1000, 3)\n",
    "pseudolabels = torch.softmax(pseudolabels, -1) if soft_labels else torch.argmax(pseudolabels, -1)\n",
    "hpl = torch.argmax(pseudolabels, -1) if soft_labels else pseudolabels\n",
    "\n",
    "if filter_pseudolabels and pseudolabels.ndim > 1: \n",
    "    error = 1 - pseudolabels.max(-1).values\n",
    "    filt_pl_idx = np.arange(len(error))[error < filter_pseudolabels]\n",
    "    filt_pl = pseudolabels[error < filter_pseudolabels]\n",
    "    assert len(filt_pl) > 0, 'no filtered pseudolabels'\n",
    "    filt_hpl = torch.argmax(filt_pl, -1)\n",
    "else: \n",
    "    filt_pl_idx = np.arange(len(pseudolabels))\n",
    "    filt_pl = filt_hpl = pseudolabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_split = filt_pl_idx[balance_idx(filt_hpl)] if balanced_pseudolabels else filt_pl_idx\n",
    "test_eq(hpl[pl_split].float().mean(), np.mean(np.unique(hpl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def TrainValidTestSplitter(n_splits:int=1, valid_size:Union[float, int]=0.2, test_size:Union[float, int]=0., train_only:bool=False,\n",
    "                           stratify:bool=True, balance:bool=False, strategy:str=\"oversample\", shuffle:bool=True, \n",
    "                           random_state:Union[None, int]=None, verbose:bool=False, **kwargs):\n",
    "    \"Split `items` into random train, valid (and test optional) subsets.\"\n",
    "    \n",
    "    if not shuffle and stratify and not train_only: \n",
    "        pv('stratify set to False because shuffle=False. If you want to stratify set shuffle=True', verbose)\n",
    "        stratify = False\n",
    "        \n",
    "    def _inner(o, **kwargs):\n",
    "        if stratify:\n",
    "            _, unique_counts = np.unique(o, return_counts=True)\n",
    "            if np.min(unique_counts) >= 2 and np.min(unique_counts) >= n_splits: stratify_ = stratify  \n",
    "            elif np.min(unique_counts) < n_splits: \n",
    "                stratify_ = False\n",
    "                pv(f'stratify set to False as n_splits={n_splits} cannot be greater than the min number of members in each class ({np.min(unique_counts)}).', \n",
    "                   verbose)\n",
    "            else:\n",
    "                stratify_ = False\n",
    "                pv('stratify set to False as the least populated class in o has only 1 member, which is too few.', verbose)\n",
    "        else: stratify_ = False\n",
    "        vs = 0 if train_only else 1. / n_splits if n_splits > 1 else int(valid_size * len(o)) if isinstance(valid_size, float) else valid_size\n",
    "        if test_size: \n",
    "            ts = int(test_size * len(o)) if isinstance(test_size, float) else test_size\n",
    "            train_valid, test = train_test_split(range(len(o)), test_size=ts, stratify=o if stratify_ else None, shuffle=shuffle, \n",
    "                                                 random_state=random_state, **kwargs)\n",
    "            test = toL(test)\n",
    "            if shuffle: test = random_shuffle(test, random_state)\n",
    "            if vs == 0:\n",
    "                train, _ = RandomSplitter(0, seed=random_state)(o[train_valid])\n",
    "                train = toL(train)\n",
    "                if balance: train = train[balance_idx(o[train], random_state=random_state, strategy=strategy)]\n",
    "                if shuffle: train = random_shuffle(train, random_state)\n",
    "                train_ = L(L([train]) * n_splits) if n_splits > 1 else train\n",
    "                valid_ = L(L([train]) * n_splits) if n_splits > 1 else train\n",
    "                test_ = L(L([test]) * n_splits) if n_splits > 1 else test\n",
    "                if n_splits > 1: \n",
    "                    return [split for split in itemify(train_, valid_, test_)]\n",
    "                else: \n",
    "                    return train_, valid_, test_\n",
    "            elif n_splits > 1: \n",
    "                if stratify_: \n",
    "                    splits = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state).split(np.arange(len(train_valid)), o[train_valid])\n",
    "                else:\n",
    "                    splits = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state).split(np.arange(len(train_valid)))\n",
    "                train_, valid_ = L([]), L([])\n",
    "                for train, valid in splits:\n",
    "                    train, valid = toL(train), toL(valid)\n",
    "                    if balance: train = train[balance_idx(o[train], random_state=random_state, strategy=strategy)]\n",
    "                    if shuffle: \n",
    "                        train = random_shuffle(train, random_state)\n",
    "                        valid = random_shuffle(valid, random_state)\n",
    "                    train_.append(L(L(train_valid)[train]))\n",
    "                    valid_.append(L(L(train_valid)[valid]))\n",
    "                test_ = L(L([test]) * n_splits)\n",
    "                return [split for split in itemify(train_, valid_, test_)]\n",
    "            else:\n",
    "                train, valid = train_test_split(range(len(train_valid)), test_size=vs, random_state=random_state, \n",
    "                                                stratify=o[train_valid] if stratify_ else None, shuffle=shuffle, **kwargs)\n",
    "                train, valid = toL(train), toL(valid)\n",
    "                if balance: train = train[balance_idx(o[train], random_state=random_state, strategy=strategy)]\n",
    "                if shuffle: \n",
    "                    train = random_shuffle(train, random_state)\n",
    "                    valid = random_shuffle(valid, random_state)\n",
    "                return (L(L(train_valid)[train]), L(L(train_valid)[valid]),  test)\n",
    "        else: \n",
    "            if vs == 0:\n",
    "                train, _ = RandomSplitter(0, seed=random_state)(o)\n",
    "                train = toL(train)\n",
    "                if balance: train = train[balance_idx(o[train], random_state=random_state, strategy=strategy)]\n",
    "                if shuffle: train = random_shuffle(train, random_state)\n",
    "                train_ = L(L([train]) * n_splits) if n_splits > 1 else train\n",
    "                valid_ = L(L([train]) * n_splits) if n_splits > 1 else train\n",
    "                if n_splits > 1: \n",
    "                    return [split for split in itemify(train_, valid_)]\n",
    "                else: \n",
    "                    return (train_, valid_)\n",
    "            elif n_splits > 1: \n",
    "                if stratify_: splits = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state).split(np.arange(len(o)), o)\n",
    "                else: splits = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state).split(np.arange(len(o)))\n",
    "                train_, valid_ = L([]), L([])\n",
    "                for train, valid in splits:\n",
    "                    train, valid = toL(train), toL(valid)\n",
    "                    if balance: train = train[balance_idx(o[train], random_state=random_state, strategy=strategy)]\n",
    "                    if shuffle: \n",
    "                        train = random_shuffle(train, random_state)\n",
    "                        valid = random_shuffle(valid, random_state)\n",
    "                    if not isinstance(train, (list, L)):  train = train.tolist()\n",
    "                    if not isinstance(valid, (list, L)):  valid = valid.tolist()\n",
    "                    train_.append(L(train))\n",
    "                    valid_.append(L(L(valid)))\n",
    "                return [split for split in itemify(train_, valid_)]\n",
    "            else:\n",
    "                train, valid = train_test_split(range(len(o)), test_size=vs, random_state=random_state, stratify=o if stratify_ else None, \n",
    "                                                shuffle=shuffle, **kwargs)\n",
    "                train, valid = toL(train), toL(valid)\n",
    "                if balance: train = train[balance_idx(o[train], random_state=random_state, strategy=strategy)]\n",
    "                return train, valid\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def plot_splits(splits, contains_test_data):\n",
    "    _max = 0\n",
    "    _splits = 0\n",
    "    for i, split in enumerate(splits):\n",
    "        if is_listy(split[0]):\n",
    "            for j, s in enumerate(split):\n",
    "                _max = max(_max, array(s).max())\n",
    "                _splits += 1\n",
    "        else: \n",
    "            _max = max(_max, array(split).max())\n",
    "            _splits += 1\n",
    "    _splits = [splits] if not is_listy(split[0]) else splits\n",
    "    v = np.zeros((len(_splits), _max + 1))\n",
    "    for i, split in enumerate(_splits):\n",
    "        if is_listy(split[0]):\n",
    "            for j, s in enumerate(split): \n",
    "                v[i, s] = 1 + j\n",
    "        else: v[i, split] = 1 + i\n",
    "    vals = np.unique(v)\n",
    "    if 2 in vals and 3 not in vals and contains_test_data:\n",
    "        vals = [v + 1 if v == 2 else v for v in vals]\n",
    "    plt.figure(figsize=(16, len(_splits)/2))\n",
    "    if len(vals) == 1:\n",
    "        v = np.ones((len(_splits), _max + 1))\n",
    "        plt.pcolormesh(v, color='blue')\n",
    "        legend_elements = [Patch(facecolor='blue', label='Train')]\n",
    "        plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else: \n",
    "        colors = L(['gainsboro', 'blue', 'orange', 'limegreen'])[vals]\n",
    "        cmap = LinearSegmentedColormap.from_list('', colors)\n",
    "        plt.pcolormesh(v, cmap=cmap)\n",
    "        legend_elements = L([\n",
    "            Patch(facecolor='gainsboro', label='None'),\n",
    "            Patch(facecolor='blue', label='Train'),\n",
    "            Patch(facecolor='orange', label='Valid'),\n",
    "            Patch(facecolor='limegreen', label='Test')])[vals]\n",
    "        plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title('Split distribution')\n",
    "    plt.yticks(ticks=np.arange(.5, len(_splits)+.5, 1.0), labels=np.arange(1, len(_splits)+1, 1.0).astype(int))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_splits(o, n_splits:int=1, valid_size:float=0.2, test_size:float=0., train_only:bool=False, train_size:Union[None, float, int]=None, balance:bool=False,\n",
    "               strategy:str=\"oversample\", shuffle:bool=True, stratify:bool=True, check_splits:bool=True, random_state:Union[None, int]=None, \n",
    "               show_plot:bool=True, verbose:bool=False):\n",
    "    '''Arguments: \n",
    "        o            : object to which splits will be applied, usually target.\n",
    "        n_splits     : number of folds. Must be an int >= 1.\n",
    "        valid_size   : size of validation set. Only used if n_splits = 1. If n_splits > 1 valid_size = (1. - test_size) / n_splits. \n",
    "        test_size    : size of test set. Default = 0.\n",
    "        train_only   : if True valid set == train set. This may be useful for debugging purposes.\n",
    "        train_size   : size of the train set used. Default = None (the remainder after assigning both valid and test). \n",
    "                        Useful for to get learning curves with different train sizes or get a small batch to debug a neural net.\n",
    "        balance      : whether to balance data so that train always contain the same number of items per class.\n",
    "        strategy     : strategy to balance data (\"undersample\" or \"oversample\"). Default = \"oversample\".\n",
    "        shuffle      : whether to shuffle data before splitting into batches. Note that the samples within each split will be shuffle.\n",
    "        stratify     : whether to create folds preserving the percentage of samples for each class.\n",
    "        check_splits : whether to perform leakage and completion checks.\n",
    "        random_state : when shuffle is True, random_state affects the ordering of the indices. Pass an int for reproducible output.\n",
    "        show_plot    : plot the split distribution\n",
    "    '''\n",
    "    if n_splits == 1 and valid_size == 0. and  test_size == 0.: train_only = True\n",
    "    if balance: stratify = True\n",
    "    splits = TrainValidTestSplitter(n_splits, valid_size=valid_size, test_size=test_size, train_only=train_only, stratify=stratify, \n",
    "                                    balance=balance, strategy=strategy, shuffle=shuffle, random_state=random_state, verbose=verbose)(o)\n",
    "    if check_splits:\n",
    "        if train_only or (n_splits == 1 and valid_size == 0): print('valid == train')\n",
    "        elif n_splits > 1: \n",
    "            for i in range(n_splits): \n",
    "                leakage_finder([*splits[i]], verbose=True)\n",
    "                cum_len = 0\n",
    "                for split in splits[i]: cum_len += len(split)\n",
    "                if not balance: assert len(o) == cum_len, f'len(o)={len(o)} while cum_len={cum_len}'\n",
    "        else: \n",
    "            leakage_finder([splits], verbose=True)\n",
    "            cum_len = 0\n",
    "            if not isinstance(splits[0], Integral):\n",
    "                for split in splits: cum_len += len(split)\n",
    "            else: cum_len += len(splits)\n",
    "            if not balance: assert len(o) == cum_len, f'len(o)={len(o)} while cum_len={cum_len}'\n",
    "    if train_size is not None and train_size != 1: # train_size=1 legacy\n",
    "        if n_splits > 1:\n",
    "            splits = list(splits)\n",
    "            for i in range(n_splits): \n",
    "                splits[i] = list(splits[i])\n",
    "                if isinstance(train_size, Integral):\n",
    "                    n_train_samples = train_size  \n",
    "                elif train_size > 0 and train_size < 1: \n",
    "                    n_train_samples = int(len(splits[i][0]) * train_size)\n",
    "                splits[i][0] = L(random_choice(splits[i][0], n_train_samples, False).tolist())\n",
    "                if train_only:\n",
    "                    if valid_size != 0: splits[i][1] = splits[i][0]\n",
    "                    if test_size != 0: splits[i][2] = splits[i][0]\n",
    "                splits[i] = tuple(splits[i])\n",
    "            splits = tuple(splits)\n",
    "        else: \n",
    "            splits = list(splits)\n",
    "            if isinstance(train_size, Integral):\n",
    "                n_train_samples = train_size  \n",
    "            elif train_size > 0 and train_size < 1: \n",
    "                n_train_samples = int(len(splits[0]) * train_size)\n",
    "            splits[0] = L(random_choice(splits[0], n_train_samples, False).tolist())\n",
    "            if train_only:\n",
    "                if valid_size != 0: splits[1] = splits[0]\n",
    "                if test_size != 0: splits[2] = splits[0]\n",
    "            splits = tuple(splits)\n",
    "    if show_plot: plot_splits(splits, test_size > 0)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits                = 5\n",
    "valid_size              = 0.2\n",
    "test_size               = 0.2\n",
    "train_only              = False  # set to True for debugging (valid = train)\n",
    "train_size              = 5000\n",
    "stratify                = True\n",
    "balance                 = False\n",
    "shuffle                 = True\n",
    "predefined_splits       = None\n",
    "show_plot               = True \n",
    "\n",
    "\n",
    "check_splits = True\n",
    "random_state = 23\n",
    "\n",
    "y = np.random.randint(0, 3, 10000) + 100\n",
    "\n",
    "splits = get_splits(y, n_splits=n_splits, valid_size=valid_size, test_size=test_size, shuffle=shuffle, balance=balance, stratify=stratify,\n",
    "                    train_only=train_only, train_size=train_size, check_splits=check_splits, random_state=random_state, show_plot=show_plot, verbose=True)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=256\n",
    "y = np.random.randint(0, 3, 1000) + 100\n",
    "splits = get_splits(y, train_size=train_size, train_only=True)\n",
    "test_eq(splits[0], splits[1])\n",
    "test_eq(len(splits[0]), train_size)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_walk_forward_splits(\n",
    "    o, # 3D object with shape [samples x features x steps] containing the time series we need to split\n",
    "    n_splits=1, # # of splits\n",
    "    train_size=None, # optional: training set size as an int or a float. None when using and anchored strategy.\n",
    "    valid_size=0.2, # validation set size as an int or a float\n",
    "    test_size=0., # test set size as an int or a float\n",
    "    anchored = False, # starting point for train set remains the same for all splits\n",
    "    gap = 0., # # of samples to exclude from the end of each train set before the validation set. Entered as an int or a float\n",
    "    test_after_valid = True, # flag to indicate if validation and test will be samples randomly or sequentially\n",
    "    random_state = None, # integer that can be used to generate reproducible results\n",
    "    show_plot=True, # plots the splits created\n",
    "):\n",
    "\n",
    "    if anchored:\n",
    "        train_size = None\n",
    "    elif isinstance(train_size, float): \n",
    "        train_size = np.int32(np.floor(len(o) * train_size))\n",
    "    if isinstance(valid_size, float): \n",
    "        valid_size = np.int32(np.floor(len(o) * valid_size))\n",
    "    if isinstance(test_size, float): \n",
    "        test_size = np.int32(np.floor(len(o) * test_size))\n",
    "    if isinstance(gap, float): \n",
    "        gap = np.int32(np.floor(len(o) * gap))\n",
    "\n",
    "    if train_size is not None:\n",
    "        assert train_size + (valid_size + test_size + gap) * n_splits <= len(o), \"reduce train_size, valid_size, test_size, gap or n_splits\"\n",
    "    else:\n",
    "        assert (valid_size + test_size + gap) * n_splits < len(o), \"reduce valid_size, test_size, gap or n_splits\"\n",
    "\n",
    "    if not test_after_valid:\n",
    "        assert valid_size == test_size\n",
    "\n",
    "    train_idxs = []\n",
    "    valid_idxs = []\n",
    "    test_idxs = []\n",
    "\n",
    "    end = 0\n",
    "    all_idxs = np.arange(len(o))\n",
    "    for n in range(n_splits):\n",
    "        if valid_size > 0 and test_size > 0:\n",
    "            if test_after_valid:\n",
    "                test_idxs.append(L(all_idxs[-test_size:].tolist()))\n",
    "                all_idxs = all_idxs[:-test_size]\n",
    "                valid_idxs.append(L(all_idxs[-valid_size:].tolist()))\n",
    "                all_idxs = all_idxs[:-valid_size]\n",
    "                if gap > 0:\n",
    "                    all_idxs = all_idxs[:-gap]\n",
    "                if anchored:\n",
    "                    train_idxs.append(L(all_idxs.tolist()))\n",
    "                else:\n",
    "                    train_idxs.append(L(all_idxs[-train_size:].tolist()))\n",
    "            else:\n",
    "                valid_test_idxs = all_idxs[-test_size - valid_size:]\n",
    "                np.random.seed(random_state)\n",
    "                valid_test_idxs = np.random.permutation(valid_test_idxs)\n",
    "                valid_idxs.append(L(valid_test_idxs[:valid_size]))\n",
    "                test_idxs.append(L(valid_test_idxs[valid_size:]))\n",
    "                all_idxs = all_idxs[:-test_size - valid_size]\n",
    "                if gap > 0:\n",
    "                    all_idxs = all_idxs[:-gap]\n",
    "                if anchored:\n",
    "                    train_idxs.append(L(all_idxs.tolist()))\n",
    "                else:\n",
    "                    train_idxs.append(L(all_idxs[-train_size:].tolist()))\n",
    "        elif valid_size > 0:\n",
    "            valid_idxs.append(L(all_idxs[-valid_size:].tolist()))\n",
    "            all_idxs = all_idxs[:-valid_size]\n",
    "            test_idxs.append(L([]))\n",
    "            if gap > 0:\n",
    "                all_idxs = all_idxs[:-gap]\n",
    "            if anchored:\n",
    "                train_idxs.append(L(all_idxs.tolist()))\n",
    "            else:\n",
    "                train_idxs.append(L(all_idxs[-train_size:].tolist()))\n",
    "\n",
    "    splits = []\n",
    "    for n in range(n_splits):\n",
    "        if valid_size > 0 and test_size > 0:\n",
    "            splits.append((L(train_idxs[n]), L(valid_idxs[n]), L(test_idxs[n])))\n",
    "        elif valid_size > 0:\n",
    "            splits.append((L(train_idxs[n]), L(valid_idxs[n])))\n",
    "        else:\n",
    "            splits.append((L(train_idxs[n]),))\n",
    "    splits = tuple(splits)[::-1]\n",
    "    if show_plot:\n",
    "        plot_splits(splits, test_size > 0)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = np.random.rand(10_000, 3,  50) # shape: [samples x features x steps]\n",
    "\n",
    "splits = get_walk_forward_splits(\n",
    "    o, \n",
    "    n_splits=4, \n",
    "    train_size=.6,\n",
    "    valid_size=0.1, \n",
    "    test_size=0.1, \n",
    "    anchored = True,\n",
    "    gap = 100,\n",
    "    test_after_valid = True,\n",
    "    random_state = None,\n",
    "    show_plot=True,\n",
    ")\n",
    "\n",
    "splits = get_walk_forward_splits(\n",
    "    o, \n",
    "    n_splits=3, \n",
    "    train_size=0.3,\n",
    "    valid_size=0.1, \n",
    "    test_size=0.1, \n",
    "    anchored = False,\n",
    "    gap = 0.,\n",
    "    test_after_valid = False,\n",
    "    random_state = None,\n",
    "    show_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def TSSplitter(\n",
    "    valid_size=0.2, # int or float indicating the validation set size\n",
    "    test_size=0., # int or float indicating the test set size\n",
    "    fcst_horizon=0, # int that indicates the number of time steps removed at the end of train (and validation)\n",
    "    show_plot=True, # flag that indicates if a plot showing the splits will be created\n",
    "):\n",
    "    \"Create function that splits `items` between train/val with `valid_size` without shuffling data.\"\n",
    "    \n",
    "    if fcst_horizon: \n",
    "        fcst_horizon = fcst_horizon - 1\n",
    "        \n",
    "    def _inner(o):\n",
    "        valid_cut = valid_size if isinstance(valid_size, Integral) else round(valid_size * len(o))\n",
    "        if test_size: \n",
    "            test_cut = test_size if isinstance(test_size, Integral) else round(test_size * len(o))\n",
    "        else:\n",
    "            test_cut = 0\n",
    "        idx = np.arange(len(o), dtype=smallest_dtype(len(o)))\n",
    "        if test_size: \n",
    "            if len(idx) < 1_000_000:\n",
    "                splits = (L(idx[:-valid_cut - test_cut - fcst_horizon].tolist()), \n",
    "                          L(idx[-valid_cut - test_cut: - test_cut - fcst_horizon].tolist()),\n",
    "                          L(idx[-test_cut:].tolist()))\n",
    "            else:\n",
    "                splits = (idx[:-valid_cut - test_cut - fcst_horizon], \n",
    "                          idx[-valid_cut - test_cut: - test_cut - fcst_horizon],\n",
    "                          idx[-test_cut:])\n",
    "        else: \n",
    "            if len(idx) < 1_000_000:\n",
    "                splits = (L(idx[:-valid_cut - fcst_horizon].tolist()), L(idx[-valid_cut:].tolist()))\n",
    "            else:\n",
    "                splits = (idx[:-valid_cut - fcst_horizon], idx[-valid_cut:])\n",
    "        if show_plot: \n",
    "            if len(o) > 1_000_000:\n",
    "                warnings.warn('the splits are too large to be plotted')\n",
    "            else: \n",
    "                plot_splits(splits, test_size > 0) if test_size else plot_splits(splits[:2], test_size > 0)\n",
    "        return splits\n",
    "    return _inner\n",
    "\n",
    "TimeSplitter = TSSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(1000) + 100\n",
    "test_eq(TimeSplitter(valid_size=0.2)(y)[1], L(np.arange(800, 1000).tolist()))\n",
    "test_eq(TimeSplitter(valid_size=0.2)(y)[0], TimeSplitter(valid_size=200)(y)[0])\n",
    "TimeSplitter(valid_size=0.2, show_plot=True)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits                = 5\n",
    "valid_size              = 0.2  \n",
    "test_size               = 0\n",
    "train_only              = False  # set to True for debugging (valid = train)\n",
    "train_size              = None\n",
    "stratify                = True\n",
    "balance                 = True\n",
    "shuffle                 = True\n",
    "predefined_splits       = None\n",
    "show_plot               = True \n",
    "\n",
    "\n",
    "check_splits = True\n",
    "random_state = 23\n",
    "\n",
    "splits = get_splits(y, n_splits=n_splits, valid_size=valid_size, test_size=test_size, shuffle=shuffle, balance=balance, stratify=stratify,\n",
    "                    train_only=train_only, train_size=train_size, check_splits=check_splits, random_state=random_state, show_plot=show_plot, verbose=True)\n",
    "split = splits[0] if n_splits == 1 else splits[0][0]\n",
    "y[split].mean(), split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list([splits[0], splits[1], splits[2], splits[3], splits[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "valid_size = 0.\n",
    "test_size = 0.\n",
    "shuffle = True\n",
    "stratify = True\n",
    "train_only = True\n",
    "train_size = None\n",
    "check_splits = True\n",
    "random_state = 1\n",
    "show_plot = True \n",
    "\n",
    "splits = get_splits(y, n_splits=n_splits, valid_size=valid_size, test_size=test_size, shuffle=shuffle, stratify=stratify,\n",
    "                    train_only=train_only, train_size=train_size, check_splits=check_splits, random_state=random_state, show_plot=show_plot, verbose=True)\n",
    "for split in splits: \n",
    "    test_eq(len(split[0]), len(y))\n",
    "    test_eq(np.sort(split[0]), np.arange(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "\n",
    "splits = get_splits(y, n_splits=n_splits, shuffle=False, check_splits=True)\n",
    "test_eq(np.concatenate((L(zip(*splits))[1])), np.arange(len(y)))\n",
    "\n",
    "splits = get_splits(y, n_splits=n_splits, shuffle=True, check_splits=True)\n",
    "test_eq(np.sort(np.concatenate((L(zip(*splits))[1]))), np.arange(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 2\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "\n",
    "splits = get_splits(y, n_splits=n_splits, test_size=0.2, shuffle=False)\n",
    "for i in range(n_splits): leakage_finder(*splits[i])\n",
    "test_eq(len(splits), n_splits)\n",
    "test_eq(len(splits[0]), 3)\n",
    "s = []\n",
    "[s.extend(split) for split in splits[0]]\n",
    "test_eq(np.sort(s), np.arange(len(y)))\n",
    "s = []\n",
    "[s.extend(split) for split in splits[1]]\n",
    "test_eq(np.sort(s), np.arange(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 2, 1000)\n",
    "splits1 = get_splits(y, valid_size=.25, test_size=0, random_state=23, stratify=True, shuffle=True)\n",
    "splits2 = get_splits(y, valid_size=.25, test_size=0, random_state=23, stratify=True, shuffle=True)\n",
    "splits3 = get_splits(y, valid_size=.25, test_size=0, random_state=None, stratify=True, shuffle=True)\n",
    "splits4 = get_splits(y, valid_size=.25, test_size=0, random_state=None, stratify=True, shuffle=True)\n",
    "test_eq(splits1[0], splits2[0])\n",
    "test_ne(splits3[0], splits4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 2, 100)\n",
    "splits = get_splits(y, valid_size=.25, test_size=0, random_state=23, stratify=True, shuffle=True)\n",
    "test_eq(len(splits), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 2, 100)\n",
    "splits = get_splits(y, valid_size=.25, test_size=0, random_state=23, stratify=True)\n",
    "test_eq(len(splits), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 2, 100)\n",
    "splits = get_splits(y, valid_size=.25, test_size=20, random_state=23, stratify=True)\n",
    "test_eq(len(splits), 3)\n",
    "leakage_finder(*splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = TrainValidTestSplitter(valid_size=.25, test_size=20, random_state=23, stratify=True)(np.random.randint(0, 2, 100))\n",
    "test_eq(len(splits[1]), 25)\n",
    "test_eq(len(splits[2]), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = np.random.randint(0, 2, 1000)\n",
    "for p in [1, .75, .5, .25, .125]:\n",
    "    splits = get_splits(o, train_size=p)\n",
    "    test_eq(len(splits[0]), len(o) * .8 * p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = L([0] * 50 + [1] * 25 + [2] * 15 + [3] * 10)\n",
    "splits = get_splits(y, valid_size=.2, test_size=.2)\n",
    "test_eq(np.mean(y[splits[0]])==np.mean(y[splits[1]])==np.mean(y[splits[2]]), True)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = L([0] * 50 + [1] * 25 + [2] * 15 + [3] * 10)\n",
    "splits = get_splits(y, n_splits=1, valid_size=.2, test_size=.2, shuffle=False)\n",
    "# test_eq(splits[0] + splits[1] + splits[2], np.arange(100))\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = get_splits(np.random.randint(0,5,100), valid_size=0.213, test_size=17)\n",
    "test_eq(len(splits[1]), 21)\n",
    "test_eq(len(splits[2]), 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = get_splits(np.random.randint(0,5,100), valid_size=0.213, test_size=17, train_size=.2)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_predefined_splits(*xs):\n",
    "    '''xs is a list with X_train, X_valid, ...'''\n",
    "    splits_ = []\n",
    "    start = 0\n",
    "    for x in xs: \n",
    "        splits_.append(L(list(np.arange(start, start + len(x)))))\n",
    "        start += len(x)\n",
    "    return tuple(splits_)\n",
    "\n",
    "def combine_split_data(xs, ys=None):\n",
    "    '''xs is a list with X_train, X_valid, .... ys is None or a list with y_train, y_valid, .... '''\n",
    "    xs = [to3d(x) for x in xs]\n",
    "    splits = get_predefined_splits(*xs)\n",
    "    if ys is None: return concat(*xs), None, splits\n",
    "    else: return concat(*xs), concat(*ys), splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_splits_len(splits):\n",
    "    _len = []\n",
    "    for split in splits: \n",
    "        if isinstance(split[0], (list, L, tuple)):  _len.append([len(s) for s in split])\n",
    "        else: _len.append(len(split))\n",
    "    return _len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = np.random.rand(3,3,4), np.random.randint(0,2,3), np.random.rand(2,3,4), np.random.randint(0,2,2)\n",
    "X, y, splits = combine_split_data([X_train, X_valid], [y_train, y_valid])\n",
    "test_eq(X_train, X[splits[0]])\n",
    "test_eq(X_valid, X[splits[1]])\n",
    "test_type(X_train, X)\n",
    "test_type(y_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = np.random.rand(3,4), np.random.randint(0,2,3), np.random.rand(2,4), np.random.randint(0,2,2)\n",
    "X, y, splits = combine_split_data([X_train, X_valid], [y_train, y_valid])\n",
    "test_eq(X_train[:, None], X[splits[0]])\n",
    "test_eq(X_valid[:, None], X[splits[1]])\n",
    "test_type(X_train, X)\n",
    "test_type(y_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_usable_idxs(df, fcst_history, fcst_horizon, stride=1):\n",
    "    if len(df) < fcst_history + fcst_horizon:\n",
    "        return np.array([], dtype=int)\n",
    "    usable_idxs = df[fcst_history - 1:len(df) - fcst_horizon].index.values\n",
    "    if stride != 1:\n",
    "        usable_idxs = usable_idxs[::-stride][::-1]\n",
    "    return usable_idxs\n",
    "\n",
    "\n",
    "def get_df_usable_idxs(\n",
    "    df,                         # dataframe containing a sorted time series\n",
    "    fcst_history,               # # historical steps used as input (size of the sliding window for the input)\n",
    "    fcst_horizon,               # # steps forecasted into the future (size of the sliding window for the target)\n",
    "    stride=1,                   # int or tuple of 2 int containing the strides of the sliding windows (input and target)\n",
    "    unique_id_cols=None,        # str indicating the column/s with the unique identifier/s for each entity\n",
    "    return_np_indices=False,    # bool indicating what type of indices are returned. Default to False (dataframe indices)\n",
    "):\n",
    "    \"Calculates the indices that can be used from a df when using a sliding window\"\n",
    "    \n",
    "    dtype = smallest_dtype(len(df))\n",
    "    if unique_id_cols is not None:\n",
    "        usable_df_idxs = np.sort(np.concatenate(df\n",
    "                                                .reset_index(drop=True)\n",
    "                                                .groupby(unique_id_cols)\n",
    "                                                .apply(lambda x: get_usable_idxs(x, \n",
    "                                                                                 fcst_history=fcst_history, \n",
    "                                                                                 fcst_horizon=fcst_horizon, \n",
    "                                                                                 stride=stride\n",
    "                                                                                )).values, dtype=dtype))\n",
    "    else:\n",
    "        usable_df_idxs = np.sort(get_usable_idxs(df, fcst_history, fcst_horizon, stride).astype(dtype=dtype))\n",
    "    if return_np_indices:\n",
    "        usable_df_idxs = usable_df_idxs - (fcst_history - 1)\n",
    "    return usable_df_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def calculate_fcst_stats(\n",
    "    df, # dataframe containing a sorted time series for a single entity or subject\n",
    "    fcst_history, # # historical steps used as input.\n",
    "    fcst_horizon, # # steps forecasted into the future. \n",
    "    splits, # splits that will be used to train the model. splits[0] is the train split:\n",
    "    x_vars=None, # features used as input\n",
    "    y_vars=None,  # features used as output\n",
    "    subset_size=None, # int or float to determne the number of train samples used to calculate the mean and std\n",
    "):\n",
    "    \"Calculates the training stats required in a forecasting task\"\n",
    "    x_vars = list(df.columns) if x_vars is None else feat2list(x_vars)\n",
    "    y_vars = list(df.columns) if y_vars is None else feat2list(y_vars)\n",
    "    split = splits[0] if is_listy(splits[0]) else splits\n",
    "    if fcst_history == 1:\n",
    "        train_idxs = split\n",
    "    else:\n",
    "        \n",
    "        if subset_size is None:\n",
    "            idxs = split\n",
    "        else:\n",
    "            subset = int(subset_size) if isinstance(subset_size, Integral) else int(subset_size * len(split))\n",
    "            idxs = random_choice(idxs, subset, replace=False)\n",
    "        dtype = smallest_dtype(max(split) + fcst_history)\n",
    "        train_idxs = np.unique((np.asarray(idxs, dtype=dtype).reshape(-1,1) + np.arange(fcst_history, dtype=dtype).reshape(1, -1)).flatten())\n",
    "    mean = df.reset_index().loc[train_idxs, x_vars].mean().values.reshape(1, -1, 1)\n",
    "    std  = df.reset_index().loc[train_idxs, x_vars].std().values.reshape(1, -1, 1)\n",
    "    if x_vars == y_vars:\n",
    "        return (mean, std)\n",
    "    y_mean = df.reset_index().loc[train_idxs, y_vars].mean().values.reshape(1, -1, 1)\n",
    "    y_std  = df.reset_index().loc[train_idxs, y_vars].std().values.reshape(1, -1, 1)\n",
    "    return (mean, std), (y_mean, y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_forecasting_splits(\n",
    "    df,                         # dataframe containing a sorted time series\n",
    "    fcst_history,               # # historical steps used as input (size of the sliding window for the input)\n",
    "    fcst_horizon,               # # steps forecasted into the future (size of the sliding window for the target)\n",
    "    stride=1,                   # int or tuple of 2 int containing the strides of the sliding windows (input and target)\n",
    "    valid_size=0.,              # int or float indicating the size of the training set (based on datetimes)\n",
    "    test_size=0.2,              # int or float indicating the size of the test set (based on datetimes)\n",
    "    valid_cutoff_datetime=None, # first prediction datetime of validation dataset\n",
    "    test_cutoff_datetime=None,  # first prediction datetime of test dataset\n",
    "    datetime_col=None,          # str indicating the column with the datetime values\n",
    "    use_index=False,            # flag to indicate if the datetime is in the index\n",
    "    unique_id_cols=None,        # str indicating the column/s with the unique identifier/s for each entity\n",
    "    show_plot=True,             # flag to indicate if splits should be plotted\n",
    "):\n",
    "\n",
    "    if unique_id_cols or valid_cutoff_datetime is not None or test_cutoff_datetime is not None:\n",
    "        assert datetime_col is not None or use_index, \\\n",
    "        \"you need to pass a datetime_col or set use_index=False to be able to access datetime\"\n",
    "    \n",
    "    if valid_cutoff_datetime is not None or test_cutoff_datetime is not None:\n",
    "        valid_size = 0\n",
    "        test_size = 0\n",
    "        \n",
    "    use_valid = valid_cutoff_datetime is not None or valid_size != 0\n",
    "    use_test = test_cutoff_datetime is not None or test_size != 0\n",
    "    \n",
    "    if valid_cutoff_datetime is not None: \n",
    "        valid_cutoff_datetime = np.datetime64(valid_cutoff_datetime)\n",
    "    if test_cutoff_datetime is not None: \n",
    "        test_cutoff_datetime = np.datetime64(test_cutoff_datetime)\n",
    "\n",
    "    if use_index:\n",
    "        datetime_col = 'index' if df.index.name is None else df.index.name\n",
    "        df = df.reset_index(drop=False)[feat2list(datetime_col) + feat2list(unique_id_cols)]\n",
    "    elif datetime_col is not None:\n",
    "        df = df[feat2list(datetime_col) + feat2list(unique_id_cols)]\n",
    "    else:\n",
    "        df = df.reset_index(drop=True)\n",
    "        if unique_id_cols is not None: \n",
    "            df = df[feat2list(unique_id_cols)]\n",
    "    \n",
    "    usable_df_idxs = get_df_usable_idxs(df, fcst_history, fcst_horizon, stride=stride, unique_id_cols=unique_id_cols)\n",
    "    usable_np_idxs = usable_df_idxs - (fcst_history - 1)\n",
    "\n",
    "    if datetime_col is not None:\n",
    "        usable_steps = pd.to_datetime(df.loc[usable_df_idxs, datetime_col])\n",
    "        cat = usable_steps.astype('category').cat\n",
    "        usable_step_codes = cat.codes.values\n",
    "    else:\n",
    "        usable_step_codes = np.arange(len(usable_df_idxs))\n",
    "        \n",
    "\n",
    "    # test indices\n",
    "    if test_cutoff_datetime is not None:\n",
    "        test_start = np.argmax(cat.categories >= test_cutoff_datetime)\n",
    "        test_idxs = usable_np_idxs[usable_step_codes >= test_start]\n",
    "    elif test_size:\n",
    "        if test_size < 1:\n",
    "            if unique_id_cols is None:\n",
    "                n_usable_steps = len(usable_step_codes) - (fcst_horizon - 1) * (int(valid_size > 0) + int(test_size > 0))\n",
    "            else:\n",
    "                n_usable_steps = len(usable_step_codes)\n",
    "            test_size = round(n_usable_steps * test_size)\n",
    "        test_start = np.sort(usable_step_codes)[- test_size]\n",
    "        test_idxs = usable_np_idxs[usable_step_codes >= test_start]\n",
    "    else:\n",
    "        test_idxs = np.array([])\n",
    "    test_size = len(test_idxs)\n",
    "    \n",
    "    # valid indices\n",
    "    if valid_cutoff_datetime is not None:\n",
    "        valid_start =  np.argmax(cat.categories >= valid_cutoff_datetime)\n",
    "        if test_cutoff_datetime is not None:\n",
    "            valid_end = test_start - (fcst_horizon - 1) // stride\n",
    "            assert valid_start <= valid_end, \"you need to modify valid_size and/or test_size due to lack of data\"\n",
    "            valid_idxs = usable_np_idxs[(usable_step_codes >= valid_start) & (usable_step_codes < valid_end)]\n",
    "        else:\n",
    "            valid_idxs = usable_np_idxs[(usable_step_codes >= valid_start)]\n",
    "    elif valid_size:\n",
    "        if valid_size < 1:\n",
    "            if unique_id_cols is None:\n",
    "                n_usable_steps = len(usable_step_codes) - (fcst_horizon - 1) * (int(valid_size > 0) + int(test_size > 0))\n",
    "            else:\n",
    "                n_usable_steps = len(usable_step_codes)\n",
    "            valid_size = round(n_usable_steps * valid_size)\n",
    "        if test_size:\n",
    "            valid_end = test_start - (fcst_horizon - 1) // stride\n",
    "            remaining_usable_step_codes = usable_step_codes[usable_step_codes < valid_end]\n",
    "            valid_start = np.sort(remaining_usable_step_codes)[- valid_size]\n",
    "            assert 0 < valid_start <= valid_end <= test_start, \"you need to modify valid_size and/or test_size due to lack of data\"\n",
    "            valid_idxs = usable_np_idxs[(usable_step_codes >= valid_start) & (usable_step_codes < valid_end)]\n",
    "        else:\n",
    "            valid_start = np.sort(usable_step_codes)[- valid_size]\n",
    "            valid_idxs = usable_np_idxs[usable_step_codes >= valid_start]\n",
    "    else:\n",
    "        valid_idxs = np.array([])\n",
    "    valid_size = len(valid_idxs)\n",
    "\n",
    "    # train indices\n",
    "    if use_valid:\n",
    "        train_end = valid_start - (fcst_horizon - 1) // stride\n",
    "        assert train_end > 0, \"you need to modify valid_size due to lack of data\"\n",
    "        train_idxs = usable_np_idxs[usable_step_codes < train_end]\n",
    "    elif use_test:\n",
    "        train_end = test_start - (fcst_horizon - 1) // stride\n",
    "        assert train_end > 0, \"you need to modify test_size due to lack of data\"\n",
    "        train_idxs = usable_np_idxs[usable_step_codes < train_end]\n",
    "    else:\n",
    "        train_idxs = usable_np_idxs\n",
    "    train_size = len(train_idxs)\n",
    "\n",
    "    \n",
    "    if len(df) < 1_000_000:\n",
    "        train_idxs = L(train_idxs.tolist())\n",
    "        if len(valid_idxs):\n",
    "            valid_idxs = L(valid_idxs.tolist())\n",
    "        if len(test_idxs):\n",
    "            test_idxs = L(test_idxs.tolist())\n",
    "\n",
    "    splits = (train_idxs,)\n",
    "    if valid_size:\n",
    "        splits += (valid_idxs,)\n",
    "    if test_size:\n",
    "        splits += (test_idxs,)\n",
    "\n",
    "    if show_plot:\n",
    "        if len(df) > 1_000_000:\n",
    "            warnings.warn('the splits are too large to be plotted')\n",
    "        else:\n",
    "            plot_splits(splits, test_size > 0)\n",
    "    return tuple(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_len = 100\n",
    "df2_len = 80\n",
    "\n",
    "datetime_col = 'datetime' \n",
    "df1 = pd.DataFrame(np.arange(df1_len), columns=['value'])\n",
    "df1['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df1_len, freq='1D')\n",
    "df1['type'] = 1\n",
    "\n",
    "df = df1\n",
    "display(df)\n",
    "\n",
    "# settings\n",
    "fcst_history          = 10\n",
    "fcst_horizon          = 1\n",
    "stride                = 1\n",
    "unique_id_cols        = 'type'\n",
    "datetime_col          = 'datetime' \n",
    "use_index             = False\n",
    "valid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "test_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "valid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\n",
    "test_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\n",
    "valid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "test_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "\n",
    "\n",
    "splits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n",
    "                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n",
    "                                valid_size=valid_size, test_size=test_size, \n",
    "                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n",
    "\n",
    "print(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")\n",
    "\n",
    "# settings\n",
    "fcst_history          = 10\n",
    "fcst_horizon          = 5\n",
    "stride                = 5\n",
    "unique_id_cols        = 'type'\n",
    "datetime_col          = 'datetime' \n",
    "use_index             = False\n",
    "valid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "test_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "valid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\n",
    "test_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\n",
    "valid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "test_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "\n",
    "\n",
    "splits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n",
    "                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n",
    "                                valid_size=valid_size, test_size=test_size, \n",
    "                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n",
    "\n",
    "print(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_len = 100\n",
    "df2_len = 80\n",
    "\n",
    "datetime_col = 'datetime' \n",
    "df1 = pd.DataFrame(np.arange(df1_len), columns=['value'])\n",
    "df1['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df1_len, freq='1D')\n",
    "df1['type'] = 1\n",
    "df1_index = df1.set_index(\"datetime\")\n",
    "\n",
    "df = df1_index\n",
    "display(df)\n",
    "\n",
    "# settings\n",
    "fcst_history          = 10\n",
    "fcst_horizon          = 1\n",
    "stride                = 1\n",
    "unique_id_cols        = 'type'\n",
    "datetime_col          = 'datetime' \n",
    "use_index             = True\n",
    "valid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "test_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "valid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\n",
    "test_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\n",
    "valid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "test_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "\n",
    "\n",
    "splits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n",
    "                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n",
    "                                valid_size=valid_size, test_size=test_size, \n",
    "                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n",
    "\n",
    "print(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")\n",
    "\n",
    "# settings\n",
    "fcst_history          = 10\n",
    "fcst_horizon          = 5\n",
    "stride                = 5\n",
    "unique_id_cols        = 'type'\n",
    "datetime_col          = 'datetime' \n",
    "use_index             = True\n",
    "valid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "test_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "valid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\n",
    "test_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\n",
    "valid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "test_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "\n",
    "\n",
    "splits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n",
    "                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n",
    "                                valid_size=valid_size, test_size=test_size, \n",
    "                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n",
    "\n",
    "print(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_len = 100\n",
    "df2_len = 80\n",
    "\n",
    "datetime_col = 'datetime' \n",
    "df1 = pd.DataFrame(np.arange(df1_len), columns=['value'])\n",
    "df1['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df1_len, freq='1D')\n",
    "df1['type'] = 1\n",
    "df1_index = df1.set_index(\"datetime\")\n",
    "df2 = pd.DataFrame(np.arange(df2_len) * 10, columns=['value'])\n",
    "df2['datetime'] = pd.date_range(pd.to_datetime('1749-04-15'), periods=df2_len, freq='1D')\n",
    "df2['type'] = 2\n",
    "df_comb = pd.concat([df1, df2]).reset_index(drop=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df = df_comb\n",
    "display(df)\n",
    "\n",
    "# settings\n",
    "fcst_history          = 10\n",
    "fcst_horizon          = 3\n",
    "stride                = 1\n",
    "unique_id_cols        = 'type'\n",
    "datetime_col          = 'datetime' \n",
    "use_index             = False\n",
    "valid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "test_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "valid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\n",
    "test_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\n",
    "valid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "test_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "\n",
    "\n",
    "splits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n",
    "                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n",
    "                                valid_size=valid_size, test_size=test_size, \n",
    "                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n",
    "\n",
    "print(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_len = 100\n",
    "df2_len = 80\n",
    "\n",
    "datetime_col = 'datetime' \n",
    "df1 = pd.DataFrame(np.arange(df1_len), columns=['value'])\n",
    "df1['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df1_len, freq='1D')\n",
    "df1['type'] = 1\n",
    "df1_index = df1.set_index(\"datetime\")\n",
    "df2 = pd.DataFrame(np.arange(df2_len) * 10, columns=['value'])\n",
    "df2['datetime'] = pd.date_range(pd.to_datetime('1749-04-15'), periods=df2_len, freq='1D')\n",
    "df2['type'] = 2\n",
    "df_comb = pd.concat([df1, df2]).reset_index(drop=True).reset_index(drop=True)\n",
    "df_comb_index = df_comb.set_index(\"datetime\")\n",
    "df_comb_index.index.name = None\n",
    "\n",
    "\n",
    "df = df_comb_index\n",
    "display(df)\n",
    "\n",
    "# settings\n",
    "fcst_history          = 15\n",
    "fcst_horizon          = 5\n",
    "stride                = 1\n",
    "unique_id_cols        = 'type'\n",
    "datetime_col          = 'datetime' \n",
    "use_index             = True\n",
    "valid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "test_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\n",
    "valid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\n",
    "test_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\n",
    "valid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "test_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n",
    "\n",
    "\n",
    "splits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n",
    "                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n",
    "                                valid_size=valid_size, test_size=test_size, \n",
    "                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n",
    "\n",
    "print(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_long_term_forecasting_splits(\n",
    "    df, # dataframe containing a sorted time series for a single entity or subject\n",
    "    fcst_history,   # # historical steps used as input.\n",
    "    fcst_horizon,   # # steps forecasted into the future. \n",
    "    dsid=None,      # dataset name\n",
    "    show_plot=True, # plot the splits\n",
    "):\n",
    "    \"Returns the train, valid and test splits for long-range time series datasets\"\n",
    "    \n",
    "    if dsid in [\"ETTh1\", \"ETTh2\"]:\n",
    "        border1s = [0, 12 * 30 * 24 - fcst_history, 12 * 30 * 24 + 4 * 30 * 24 - fcst_history]\n",
    "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
    "    elif dsid in [\"ETTm1\", \"ETTm2\"]:\n",
    "        border1s = [0, 12 * 30 * 24 * 4 - fcst_history, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - fcst_history]\n",
    "        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n",
    "    else:\n",
    "        train_size = .7 # default 0.7\n",
    "        test_size = .2 # default 0.2        \n",
    "        num_train = int(len(df) * train_size)\n",
    "        num_test = int(len(df) * test_size)\n",
    "        num_vali = len(df) - num_train - num_test\n",
    "        assert num_train + num_test + num_vali <= len(df)\n",
    "        border1s = [0, num_train - fcst_history, len(df) - num_test - fcst_history]\n",
    "        border2s = [num_train, num_train + num_vali, len(df)]\n",
    "\n",
    "    train_split = L(np.arange(border1s[0], border2s[0] - fcst_horizon - fcst_history + 1).tolist())\n",
    "    valid_split = L(np.arange(border1s[1], border2s[1] - fcst_horizon - fcst_history + 1).tolist())\n",
    "    test_split = L(np.arange(border1s[2], border2s[2] - fcst_horizon - fcst_history + 1).tolist())   \n",
    "    splits = train_split, valid_split, test_split\n",
    "    if show_plot:\n",
    "        plot_splits(splits)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from tsai.export import get_nb_name; nb_name = get_nb_name(locals())\n",
    "from tsai.imports import create_scripts; create_scripts(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
